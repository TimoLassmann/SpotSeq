#+TITLE:  Analysis of ENCODE K562 ChIP seq data
#+AUTHOR: Timo Lassmann
#+EMAIL:  timo.lassmann@telethonkids.org.au
#+DATE:   2018-11-27
#+LATEX_CLASS: report
#+OPTIONS:  toc:nil
#+OPTIONS: H:4
#+LATEX_CMD: pdflatex



* Preliminaries 

  This file contains all the code to run the benchmark. To get started copy this file into a benchmark directory: 

  #+BEGIN_EXAMPLE sh -n 
  BENCHDIR="$HOME/benchmarks/encode_TFBS/"
  mkdir -p $BENCHDIR
  cp ENCODE_K562_ChIP_analysis.org $BENCHDIR
  cd $BENCHDIR
  #+END_EXAMPLE


  Open the file in Emacs and tangle with =C+c C+v t=.

  Alternatively running this on the command line should work: 

  #+BEGIN_EXAMPLE sh -n 
  emacs -Q --batch \
    --eval "(progn
    (require 'org)(require 'ob)(require 'ob-tangle)
    (setq org-src-preserve-indentation t)
    (mapc (lambda (file)
    (find-file (expand-file-name file \".\"))
    (org-babel-tangle)
    (kill-buffer)) '(\"ENCODE_K562_ChIP_analysis.org\")))" 2>&1 |grep -i tangled
  #+END_EXAMPLE


  Optional: set PATH to point to directory where wims is installed. 

#+BEGIN_SRC sh 
export PATH=$PATH:$HOME/bin

#+END_SRC

* Goal 
  Apply iHMM to top peaks to see if we can: 
  a) recover known motifs 
  b) see if sequences under peaks contain additional signal 

* Wims installation 

  install with: 
  #+BEGIN_EXAMPLE sh 
  git clone https://github.com/timolassmann/spotseq.git
  ./autogen.sh 
  ./configure --bindir  $HOME/bin 
  make
  make install 
  #+END_EXAMPLE

* Install BEDtools supercite:quinlan-2014-bedtool


  #+BEGIN_SRC bash 
    echo "Installing BEDtools" 
    DIR="$HOME/programs"
    mkdir -p $DIR

    cd $DIR
    git clone https://github.com/arq5x/bedtools2.git
    cd bedtools2/
    sudo make install 

  #+END_SRC


* Setting the RNG seed globally

  This code block sets a seed globally to be used in both generating the test data sets and in wims itself.
  #+NAME: randomseed
  #+BEGIN_SRC sh -n :exports code :results none :noweb yes
    SEED=3
  #+END_SRC

  Ha! not 42. 

* Data 

  I downloaded all ENCODE ChIP-seq data using the following selection: 

  #+BEGIN_EXAMPLE 
  https://www.encodeproject.org/search/ 
  type=Experiment&status=released
  assay_slims=DNA+binding&assay_title=ChIP-seq
  award.project=ENCODE
  award.rfa=ENCODE3
  assembly=GRCh38
  replicates.library.biosample.donor.organism.scientific_name=Homo+sapiens
  target.investigated_as=transcription+factor
  biosample_type=cell+line
  cell_slims=cancer+cell
  biosample_term_name=K562
  biosample_type=cell+line 
  files.file_type=bed+narrowPeak
  #+END_EXAMPLE



  #+BEGIN_EXAMPLE html 
  https://www.encodeproject.org/search/?type=Experiment&status=released&assay_slims=DNA+binding&assay_title=ChIP-seq&award.project=ENCODE&award.rfa=ENCODE3&assembly=GRCh38&replicates.library.biosample.donor.organism.scientific_name=Homo+sapiens&target.investigated_as=transcription+factor&biosample_type=cell+line&cell_slims=cancer+cell&biosample_term_name=K562&biosample_type=cell+line&files.file_type=bed+narrowPeak

  #+END_EXAMPLE

  This will download a the list of files to be downloaded ( =files.txt= ). To start downloading the files copy the list into a raw data directory: 

  #+BEGIN_EXAMPLE bash -n

    DATADIR="$HOME/benchmarks/encode_TFBS/data"
    mkdir -p $DATADIR
    mv files.txt $DATADIR
    cd $DATADIR
  #+END_EXAMPLE
 
  and start downloading the files as per ENCODE instructions. 

  #+BEGIN_EXAMPLE bash -n
    xargs -L 1 curl -O -L < files.txt
  #+END_EXAMPLE

  However, I seem to be able to download the files faster using =wget= + =gnu parallel=supercite:tange-2011-gnu-paral. 

  #+BEGIN_EXAMPLE bash -n  
    cat files.txt | awk '{printf "wget %s\n", $1}' > wget_commands.txt
    parallel --jobs 32 < wget_commands.txt

  #+END_EXAMPLE


  All ChIP-seq runs were aligned against this reference genome: =ENCFF643CGH.tar.gz=

  To download: 

  #+BEGIN_EXAMPLE bash  -n 
    DATADIR="$HOME/benchmarks/encode_TFBS/reference_genome"
    mkdir -p $DATADIR
    cd $DATADIR
    wget https://www.encodeproject.org/files/ENCFF643CGH/@@download/ENCFF643CGH.tar.gz
    tar -xvf ENCFF643CGH.tar.gz 
  #+END_EXAMPLE

* Selection 
Let's focus on =conservative idr thresholded peaks= mapped to GRCh38. The code below prints a table containing the targeted TF, the ENCODE ID and the name of the file containing the corresponding narrow bed peaks.

 #+BEGIN_EXAMPLE bash  -n 
    DIR="$HOME/benchmarks/encode_TFBS/input_fasta"
    mkdir -p $DIR
  #+END_EXAMPLE


 #+BEGIN_SRC bash -n :tangle get_all_top_X_seq.sh :shebang #!/usr/bin/env bash 
   DIR=`pwd`
   NSEQ=

   function usage()
   {

       printf "This script will retrieve the top X sequences from a ChIP-seq bed file .\n\n" ;
       printf "usage: $0 -n <number of top regions >>\n\n" ;
       exit 1;
   }

   while getopts n:  opt
   do
       case ${opt} in
           n) NSEQ=${OPTARG};;
           ,*) usage;;
       esac
   done
   if [ "${NSEQ}" == "" ]; then usage; fi

   cat data/metadata.tsv \
       | grep GRCh38  \
       | grep conservative \
       | grep released \
       | awk 'BEGIN{FS = "\t"} {\
       printf "%s\t%s\t%s.bed.gz\n",  $1,$13,$1}' \
       | awk -v nseq="$NSEQ"  '{\
                       printf "zcat data/%s | sort  -k 7,7nr |  sort -u  -k 1,1 -k 2,2n | sort  -k 7,7nr | head -n  %d    > input_fasta/%s_%s_top_%s_regions.bed\n", $3,nseq,$2,$1,nseq;\
       }' > get_top_bed_commands.txt 

   cat data/metadata.tsv \
       | grep conservative \
       | grep GRCh38  \
       | grep released \
       | awk 'BEGIN{FS = "\t"} {\
       printf "%s\t%s\t%s.bed.gz\n",  $1,$13,$1}' \
       | awk '{\
                       printf "bedtools getfasta -fi reference_genome/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna -bed input_fasta/%s_%s_top_%s_regions.bed > input_fasta/%s_%s_top_%s_regions.fa \n",$2,$1,nseq,$2,$1,nseq;\
       }' > get_fasta_sequences_commands.txt  

   parallel --jobs 32 < get_top_bed_commands.txt 
   parallel --jobs 32 < get_fasta_sequences_commands.txt  

 #+END_SRC

* Run wims


  #+BEGIN_SRC bash -n :tangle run_wims_model.sh :shebang #!/usr/bin/env bash :noweb yes
    DIR=`pwd`
    INDIR=
    SAMPLES=10
    AVAILABLECORES=8
    JOBS=1
    function usage()
    {

        printf "This script will run wims_model in parallel on all train_*>.fa files in a target directory.\n\n" ;
        printf "usage: $0 -i <target input directory> -c <available cores (default 8)>\n\n" ;
        exit 1;
    }

    while getopts i:c:  opt
    do
        case ${opt} in
            i) INDIR=${OPTARG};;
            c) AVAILABLECORES=${OPTARG};;
            ,*) usage;;
        esac
    done
    if [ "${INDIR}" == "" ]; then usage; fi
    if [ "${AVAILABLECORES}" -lt 1 ]; then usage; fi

    INDIR=${INDIR%/}
    shopt -s nullglob
    EXISTINGFILES=($INDIR/*fa) 

    if [ "${#EXISTINGFILES[@]}" -lt 1 ]; then
        printf "\nInput directory contains no input files matching *.fa\n\n";
        exit 1;
    fi

    let JOBS="$AVAILABLECORES / 32" 

    if [ "$JOBS" -lt 1 ]; then
        JOBS=1
    fi
  #+END_SRC


  #+BEGIN_SRC bash -n :tangle run_wims_model.sh :shebang #!/usr/bin/env bash :noweb yes
    <<randomseed>>
  #+END_SRC

  #+BEGIN_SRC bash -n :tangle run_wims_model.sh :shebang #!/usr/bin/env bash :noweb yes
    printf "Will process %s jobs in parallel, each using %s cores.\n"  $JOBS 8;

    echo "find $INDIR -name *.fa | parallel --jobs $JOBS \"wims_model -i {} -o {}.h5 --nthreads 32 --niter 5000 --seed $SEED\"";
    find $INDIR -name "*.fa" | parallel --jobs $JOBS "wims_model -i {} -o {}.h5 --nthreads 32 --niter 5000  --seed $SEED"

  #+END_SRC 

* Run pair-wise score

  Run each model against each input fasta dataset. 

  1) make background fasta file

#+BEGIN_SRC  bash 
  DIR="$HOME/benchmarks/encode_TFBS/input_fasta"
  find $DIR -maxdepth 1 -iname '*.fa' -not -name 'background.fa' -exec cat {} +> $DIR/background.fa
#+END_SRC


2) run wims_score all vs all

ZZZ3-human_ENCFF129QWO

 #+BEGIN_SRC bash -n :tangle run_wims_score_all_vs_all.sh :shebang #!/usr/bin/env bash :noweb yes
      DIR=`pwd`
      INDIR=
      SAMPLES=10
      AVAILABLECORES=8
      JOBS=1
      function usage()
      {

          printf "This script will run wims_model in parallel on all train_*>.fa files in a target directory.\n\n" ;
          printf "usage: $0 -i <target input directory> -c <available cores (default 8)>\n\n" ;
          exit 1;
      }

      while getopts i:c:  opt
      do
          case ${opt} in
              i) INDIR=${OPTARG};;
              c) AVAILABLECORES=${OPTARG};;
              ,*) usage;;
          esac
      done
      if [ "${INDIR}" == "" ]; then usage; fi
      if [ "${AVAILABLECORES}" -lt 1 ]; then usage; fi

      INDIR=${INDIR%/}
      shopt -s nullglob
      MODELFILES=($INDIR/*h5) 

      if [ "${#MODELFILES[@]}" -lt 1 ]; then
          printf "\nInput directory contains no input files matching *.h5\n\n";
          exit 1;
      fi

      echo ${!MODELFILES[*]}
      printf "" > "wims_score_commands.txt"
      #printf "" > "makeROC_commands.txt"

      TESTFILES=$(sed "s/.fa.h5/.fa/g" <<< ${MODELFILES[*]})
      TESTFILES=(${TESTFILES//:/ })


      for index1 in "${!MODELFILES[@]}"
      do
          for index2 in "${!MODELFILES[@]}"
          do
              echo  "wims_score -m ${MODELFILES[$index1]} -i ${TESTFILES[$index2]} -nthreads 4 -background $INDIR/background.fa -o test.csv -s score_summary.csv " >> "wims_score_commands.txt"
          done

      done
      let JOBS="$AVAILABLECORES / 4" 

       if [ "$JOBS" -lt 1 ]; then
           JOBS=1
       fi

      parallel --jobs $JOBS < wims_score_commands.txt

  #+END_SRC


** plotting etc.. 
The script below takes log-odds scores from positive and negative test sequences, draws an area under receiver operating characteristic curve (ROC) and writes output to file. The script also plots the curves (use -d dark versions I prefer to use in presentations.


   #+BEGIN_SRC R -n :tangle makeROC.R :shebang #!/usr/bin/env Rscript :noweb yes
     library(optparse)
     sessionInfo()
     dark <- FALSE;
     error <- 0;
     option_list = list(
         make_option(c("-p", "--positive"),
                     type="character",
                     default=NULL,
                     help="scores for positive test sequences.",
                     metavar="character"),
         make_option(c("-n", "--negative"),
                     type="character",
                     default=NULL,
                     help="scores for the negative test sequences.",
                     metavar="character"),
         make_option(c("-e", "--experimentname"),
                     type="character",
                     default=NULL,
                     help="Experiment name.",
                     metavar="character"),
         make_option(c("-s", "--summary"),
                     type="character",
                     default="stats.csv",
                     help="Summary stats file name [stats.csv].",
                     metavar="character"),
         make_option(c("-k", "--error"),
                     type="integer",
                     default=0,
                     help="Errors.",
                     metavar="character"),
         make_option(c("-d", "--dark"), action="store_true", default=FALSE,
                     help="use dark theme (for presentations)")


     );

     opt_parser <- OptionParser(option_list=option_list,
                                description = "\nLoad singleR object and make plots.",
                                epilogue = "Example:\n\n  Blah  \n\n");
     opt <- parse_args(opt_parser);

     if(opt$dark){
         dark <- TRUE
     }

     error <-  opt$error;

    summaryfilename <- opt$summary

     if (is.null(opt$positive)){
         print_help(opt_parser)
         stop("Missing infile!\n", call.=FALSE)
     }
     if (is.null(opt$negative)){
         print_help(opt_parser)
         stop("Missing infile!\n", call.=FALSE)
     }

     posname <- opt$positive
     negname <- opt$negative
     name <- opt$experimentname
     pos = read.csv(posname,header = T,row.names= 1)
     neg = read.csv(negname,header = T,row.names= 1)


     <<Rlibraries>>

     response = c(rep(1,dim(pos)[1]), rep(0,dim(neg)[1]))
     predictor = c(pos[,1],neg[,1])

                                             #roc = roc(response,predictor)

     x = cbind(response,predictor)
     x = as.data.frame(x)


     if(dark){
         p = ggplot(x , aes(d = response, m = predictor))
         p <- op +  geom_roc(labels = FALSE,
                             fill=rgb(0,0,20,maxColorValue = 255),
                             color=rgb(220,210,200,maxColorValue = 255))

         p <- p + geom_abline(intercept = 0, slope = 1, color=rgb(220,210,200,maxColorValue = 255))
         p <- p +scale_x_continuous(limits = c(0,1), expand = c(0, 0))
         p <- p + scale_y_continuous(limits = c(0,1), expand = c(0, 0))

         p <- p + annotate("text",
                           color=rgb(220,210,200,maxColorValue = 255),
                           x = .75,
                           y = .25,
                           label = paste("AUC =", round(calc_auc(p)$AUC, 4)))
         p  <-  p + xlab("1-Specificity (FPR)")
         p  <-  p + ylab("Sensitivity (TPR)")

         p <- p + theme_classic()
         p <- p + theme(panel.background = element_rect(fill =rgb(0,0,20,maxColorValue = 255),colour = rgb(0,0,20,maxColorValue = 255)),
                        text = element_text(colour=rgb(220,210,200,maxColorValue = 255)),
                        rect = element_rect(fill = rgb(0,0,20,maxColorValue = 255),colour=rgb(0,0,20,maxColorValue = 255)),
                        line = element_line(colour =rgb(220,210,200,maxColorValue = 255)),
                        axis.text = element_text(colour =rgb(220,210,200,maxColorValue = 255)),
                        axis.line = element_line(colour =rgb(220,210,200,maxColorValue = 255)),
                        axis.ticks = element_line(colour = rgb(220,210,200,maxColorValue = 255)),
                        )
     }else{
         p = ggplot(x , aes(d = response, m = predictor)) + geom_roc(labels = FALSE)

         p <- p + geom_abline(intercept = 0, slope = 1)
         p <- p +scale_x_continuous(limits = c(0,1), expand = c(0, 0))
         p <- p + scale_y_continuous(limits = c(0,1), expand = c(0, 0))

         p <- p + annotate("text",
                           x = .75,
                           y = .25,
                           label = paste("AUC =", round(calc_auc(p)$AUC, 4)))
         p  <-  p + xlab("1-Specificity (FPR)")
         p  <-  p + ylab("Sensitivity (TPR)")
     }
     metadata <- tribble(~name,~error, ~AUC,
                         paste0(basename(name)), error, round(calc_auc(p)$AUC, 4))

     metadata
     if(!file.exists(summaryfilename)){
         write_csv(metadata, summaryfilename,  na = "NA", append = TRUE, col_names = TRUE)
     }else{
         write_csv(metadata, summaryfilename, na = "NA", append = TRUE, col_names = FALSE)
     }

     outname = paste0("ROC_",basename(name),".jpg");
     jpeg(outname,width = 480, height = 480, units = "px", pointsize = 12,     quality = 90)

     p




     dev.off()
     #options(tikzDocumentDeclaration = '\\documentclass{beamer}')
     #outname = paste0("ROC_",basename(name),".tex");
     #tikz(outname,width = 2, height = 2)

     #dev.off()

   #+END_SRC



** modelling: 
#+BEGIN_SRC sh
  for filename in ../input_fasta/*.fa; do
    echo "sbatch ./model_sequences.sh -i $filename -n 1000"
  done


#+END_SRC

** infoclust 
#+BEGIN_SRC sh

  for filename in ../input_fasta/*_top_1000_regions.fa.h5; do
      echo "sbatch ./infoclust.sh -m $filename "
      sbatch ./infoclust.sh -m $filename
  done


#+END_SRC
* Plot 

#+BEGIN_SRC R 

library(tidyverse)
dat = read_csv("score_summary.csv")
tmp = dcast(dat, Model~SequenceFile, value.var = 'Mean') 

mat = tmp[1:nrow(tmp),2:ncol(tmp)]; 

rownames(mat) = tmp[,1]
mat[is.na(mat)] <- 0

ggplot(dat, aes(Model, SequenceFile )) +
  geom_tile(aes(fill = Mean), color = "white")

Model                           SequenceFile                       Mean Stdev
tric versions of traditional HMMs and are cool; 

#+END_SRC
* Slurm scripts 

** model sequences 
  #+BEGIN_SRC sh :tangle model_sequences.sh :shebang #!/usr/bin/env bash

    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=16


    export PATH=/data/Projects/spotseq/bin:$PATH

    INPUT=
    NITER=

    function usage()
    {
        cat <<EOF
 usage: $0  -i <path to fasta files>  -n <number of iterations>
 EOF
        exit 1;
    }

    while getopts i:s:n:r: opt
    do
        case ${opt} in
            i) INPUT=${OPTARG};;
            n) NUMITER=${OPTARG};;
            ,*) usage;;
        esac
    done

    if [ "${INPUT}" = "" ]; then usage; fi

    #
    #   Sanity check
    #

    programs=(spotseq_model spotseq_plot spotseq_score)

    printf "Running Sanity checks:\n";

    for item in ${programs[*]}
    do
        if which $item >/dev/null; then
            printf "%15s found...\n"  $item;
        else
            printf "\nERROR: %s not found!\n\n" $item;
            exit 1;
        fi
    done

    echo "All dependencies found."
    OUTMODEL=$INPUT".h5"
    echo "spotseq_model -i $INPUT -nthreads 16 -o $OUTMODEL -niter $NUMITER -seed 42 "
    spotseq_model -i $INPUT -nthreads 16 -o $OUTMODEL -niter $NUMITER -seed 42

  #+END_SRC 

** infoclust 

   #+BEGIN_SRC sh :tangle infoclust.sh :shebang #!/usr/bin/env bash 

     #SBATCH --nodes=1
     #SBATCH --ntasks-per-node=16


     export PATH=/data/Projects/spotseq/bin:$PATH

     INPUT=
     NITER=

     function usage()
     {
         cat <<EOF
usage: $0  -m <path to model hdf5 file>  
EOF
         exit 1;
     }

     while getopts m: opt
     do
         case ${opt} in
             m) INPUT=${OPTARG};;
             ,*) usage;;
         esac
     done

     if [ "${INPUT}" = "" ]; then usage; fi

     #
     #   Sanity check
     #

     programs=(spotseq_model spotseq_plot spotseq_score spotseq_iclu)

     printf "Running Sanity checks:\n";

     for item in ${programs[*]}
     do
         if which $item >/dev/null; then
             printf "%15s found...\n"  $item;
         else
             printf "\nERROR: %s not found!\n\n" $item;
             exit 1;
         fi
     done

     echo "All dependencies found."
     OUTPUT=$INPUT"motif.h5"
     echo "spotseq_iclu -m $INPUT -out $OUTPUT "
     spotseq_iclu -m $INPUT -out $OUTPUT



   #+END_SRC 



* Sanity checks and convenience scripts

** Check for wims installation

   #+BEGIN_SRC bash -n :tangle check_for_programs.sh :shebang #!/usr/bin/env bash
     programs=(Rscript parallel pkg-config wims_model wims_score tfbs_bench bedtools zcat)

     printf "Running Sanity checks:\n";

     for item in ${programs[*]}
     do
         if which $item >/dev/null; then
             printf "%15s found.\n"  $item;
         else
             printf "\nERROR: %s not found!\n\n" $item;
             exit 1;
         fi
     done
   #+END_SRC

** Check for libraries 
   
   Here I define the list of libraries I'll be using. 
   
   #+NAME: liblist
   #+BEGIN_SRC R -n :exports code :results none
     libraries <- c("devtools","optparse","tidyverse","plotROC")
   #+END_SRC
   
   Script to test if libraries are present.
   #+BEGIN_SRC R -n :tangle test_for_libraries.R :shebang #!/usr/bin/env Rscript :noweb yes :exports code :results none
     <<liblist>>
     Sys.info()["nodename"]
     for(library in libraries) 
     { 
         f = is.element(library, installed.packages()[,1])
         print(paste("Library",library, "is installed?", f))
         if(!f)
         {
             message("Missing library:",library )
             quit(status=1)
         }
     }
     quit(status=0)
   #+END_SRC
   
   
   #+BEGIN_SRC sh -n :results output :exports both
     ./test_for_libraries.R
   #+END_SRC
   install.packages("tidyverse")
   Code block to load the libraries in R code.

   #+NAME: Rlibraries
   #+BEGIN_SRC R -n :exports code :results none :noweb yes
     <<liblist>>
     lapply(libraries, FUN = function(X) {
         do.call("library", list(X)) 
     })

   #+END_SRC

** Makefile to kick off the analysis

    1) Makefile 

#+BEGIN_SRC makefile -n :tangle Makefile
check: check_r_libs 
	@ echo Done

tangle:
	./tangleorgs.sh ENCODE_K562_ChIP_analysis.org

check_r_libs: check_programs
	@ $$(pwd)/test_for_libraries.R
	@ if [ $$? -ne 0 ]; then exit; fi;
	@ echo R libs found 

check_programs:  tangle
	@ $$(pwd)/check_for_programs.sh
	@ if [ $$? -ne 0 ]; then exit; fi;
	@ echo Programs found

#+END_SRC

** script to tangle analysis org documents from command line

#+BEGIN_SRC bash -n :tangle tangleorgs.sh :tangle-mode (identity #o700) :shebang #!/usr/bin/env bash
#
# tangle files with org-mode
#
DIR=`pwd`
FILES=""

function usage()
{
cat <<EOF

This script will:

1) tangle the input file 

usage: $0   <a.org> <b.org> ...
EOF
exit 1;
}

while getopts i:  opt
do
case ${opt} in
i) INDIR=${OPTARG};;
*) usage;;
esac
done
     
# wrap each argument in the code required to call tangle on it
for i in $@; do
   FILES="$FILES \"$i\""
done

if [ "${FILES}" = "" ]; then usage; fi
     
emacs -Q --batch \
  --eval "(progn
  (require 'org)(require 'ob)(require 'ob-tangle)
  (setq org-src-preserve-indentation t)
  (mapc (lambda (file)
  (find-file (expand-file-name file \"$DIR\"))
  (org-babel-tangle)
  (kill-buffer)) '($FILES)))" 2>&1 |grep -i tangled

#+END_SRC

* References 

#+BEGIN_SRC latex 
  \printbibliography[heading=none]

#+END_SRC

* Versions 
  #+BEGIN_SRC emacs-lisp -n :exports both :eval yes
    (princ (concat
            (format "Emacs version: %s\n"
                    (emacs-version))
            (format "org version: %s\n"
                    (org-version))))
  #+END_SRC

  #+RESULTS:
  : Emacs version: GNU Emacs 26.1 (build 1, x86_64-redhat-linux-gnu, GTK+ Version 3.22.30)
  :  of 2018-06-26
  : org version: 9.1.9

  #+BEGIN_SRC sh :results output :exports both :eval yes
    bash --version
  #+END_SRC


